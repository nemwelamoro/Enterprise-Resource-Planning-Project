#import needed libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

# Load the dataset
df = pd.read_csv("C:/Users/FBI/Desktop/Tarus project/erp dataset.csv")
df.head()

# Check the structure of the dataset
print(df.info())

# Summary statistics
print(df.describe())

# Check for missing values
print(df.isnull().sum())

#drop missing values
df = df.dropna(subset=['Work Experience'])


#Frequency distributions tables of different variables in the dataset
from collections import Counter


# Create a frequency distribution table for Level of Education
frequency_table = Counter(df['Level of Education'])

total = len(df['Level of Education'])

# Print the frequency distribution table with percentages
for level, count in frequency_table.items():
    percentage = (count / total) * 100
    print(f'{level}: {count} ({percentage:.2f}%)')


# Create a frequency distribution table Work Experience
frequency_table = Counter(df['Work Experience'])

total = len(df['Work Experience'])

# Print the frequency distribution table with percentages
for level, count in frequency_table.items():
    percentage = (count / total) * 100
    print(f'{level}: {count} ({percentage:.2f}%)')


# Create a frequency distribution table for System Navigation Statement 1 
frequency_table = Counter(df['System Navigation Statement 1'])

total = len(df['System Navigation Statement 1'])

# Print the frequency distribution table with percentages
for level, count in frequency_table.items():
    percentage = (count / total) * 100
    print(f'{level}: {count} ({percentage:.2f}%)')


# Create a frequency distribution table for System Navigation Statement 2
frequency_table = Counter(df['System Navigation Statement 2'])

total = len(df['System Navigation Statement 2'])

# Print the frequency distribution table with percentages
for level, count in frequency_table.items():
    percentage = (count / total) * 100
    print(f'{level}: {count} ({percentage:.2f}%)')


# Create a frequency distribution table for System Navigation Statement 3
frequency_table = Counter(df['System Navigation Statement 3'])

total = len(df['System Navigation Statement 3'])

# Print the frequency distribution table with percentages
for level, count in frequency_table.items():
    percentage = (count / total) * 100
    print(f'{level}: {count} ({percentage:.2f}%)')


# Create a frequency distribution table for System Navigation Statement 4
frequency_table = Counter(df['System Navigation Statement 4'])

total = len(df['System Navigation Statement 4'])

# Print the frequency distribution table with percentages
for level, count in frequency_table.items():
    percentage = (count / total) * 100
    print(f'{level}: {count} ({percentage:.2f}%)')

# Create a frequency distribution table for System Presentation Statement 1
frequency_table = Counter(df['System Presentation Statement 1'])

total = len(df['System Presentation Statement 1'])

# Print the frequency distribution table with percentages
for level, count in frequency_table.items():
    percentage = (count / total) * 100
    print(f'{level}: {count} ({percentage:.2f}%)')


# Create a frequency distribution table for System Presentation Statement 2
frequency_table = Counter(df['System Presentation Statement 2'])

total = len(df['System Presentation Statement 2'])

# Print the frequency distribution table with percentages
for level, count in frequency_table.items():
    percentage = (count / total) * 100
    print(f'{level}: {count} ({percentage:.2f}%)')


# Create a frequency distribution table for System Presentation Statement 3
frequency_table = Counter(df['System Presentation Statement 3'])

total = len(df['System Presentation Statement 3'])

# Print the frequency distribution table with percentages
for level, count in frequency_table.items():
    percentage = (count / total) * 100
    print(f'{level}: {count} ({percentage:.2f}%)')


# Create a frequency distribution table for System Presentation Statement 4
frequency_table = Counter(df['System Presentation Statement 4'])

total = len(df['System Presentation Statement 4'])

# Print the frequency distribution table with percentages
for level, count in frequency_table.items():
    percentage = (count / total) * 100
    print(f'{level}: {count} ({percentage:.2f}%)')


# Create a frequency distribution table for Security Statement 1
frequency_table = Counter(df['Security Statement 1'])

total = len(df['Security Statement 1'])

# Print the frequency distribution table with percentages
for level, count in frequency_table.items():
    percentage = (count / total) * 100
    print(f'{level}: {count} ({percentage:.2f}%)')


# Create a frequency distribution table for Security Statement 2
frequency_table = Counter(df['Security Statement 2'])

total = len(df['Security Statement 2'])

# Print the frequency distribution table with percentages
for level, count in frequency_table.items():
    percentage = (count / total) * 100
    print(f'{level}: {count} ({percentage:.2f}%)')


# Create a frequency distribution table for Security Statement 3
frequency_table = Counter(df['Security Statement 3'])

total = len(df['Security Statement 3'])

# Print the frequency distribution table with percentages
for level, count in frequency_table.items():
    percentage = (count / total) * 100
    print(f'{level}: {count} ({percentage:.2f}%)')


# Create a frequency distribution table for Security Statement 4
frequency_table = Counter(df['Security Statement 4'])

total = len(df['Security Statement 4'])

# Print the frequency distribution table with percentages
for level, count in frequency_table.items():
    percentage = (count / total) * 100
    print(f'{level}: {count} ({percentage:.2f}%)')


# Create a frequency distribution table for User Knowledge Statement 1
frequency_table = Counter(df['User Knowledge Statement 1'])

total = len(df['User Knowledge Statement 1'])

# Print the frequency distribution table with percentages
for level, count in frequency_table.items():
    percentage = (count / total) * 100
    print(f'{level}: {count} ({percentage:.2f}%)')


# Create a frequency distribution table for User Knowledge Statement 2
frequency_table = Counter(df['User Knowledge Statement 2'])

total = len(df['User Knowledge Statement 2'])

# Print the frequency distribution table with percentages
for level, count in frequency_table.items():
    percentage = (count / total) * 100
    print(f'{level}: {count} ({percentage:.2f}%)')


# Create a frequency distribution table for User Knowledge Statement 3
frequency_table = Counter(df['User Knowledge Statement 3'])

total = len(df['User Knowledge Statement 3'])

# Print the frequency distribution table with percentages
for level, count in frequency_table.items():
    percentage = (count / total) * 100
    print(f'{level}: {count} ({percentage:.2f}%)')


# Create a frequency distribution table for User Knowledge Statement 4
frequency_table = Counter(df['User Knowledge Statement 4'])

total = len(df['User Knowledge Statement 4'])

# Print the frequency distribution table with percentages
for level, count in frequency_table.items():
    percentage = (count / total) * 100
    print(f'{level}: {count} ({percentage:.2f}%)')


# Create a frequency distribution table for Academic Service Delivery
frequency_table = Counter(df['Academic Service Delivery '])

total = len(df['Academic Service Delivery '])

# Print the frequency distribution table with percentages
for level, count in frequency_table.items():
    percentage = (count / total) * 100
    print(f'{level}: {count} ({percentage:.2f}%)') 


# Visualization of academic service delivery scores
plt.figure(figsize=(10, 6))
sns.histplot(df['Academic Service Delivery '], kde=True)
plt.title('Academic Service Delivery Scores Distribution')
plt.xlabel('Score')
plt.ylabel('Frequency')
plt.show()


# Visualization of System Navigation Satisfaction
plt.figure(figsize=(10, 6))
sns.histplot(df['System Navigation Satisfaction'], kde=True)
plt.title('System Navigation Satisfaction Distribution')
plt.xlabel('Score')
plt.ylabel('Frequency')
plt.show()


# Visualization of System Navigation Satisfaction
plt.figure(figsize=(10, 6))
sns.histplot(df['System Presentation Satisfaction'], kde=True)
plt.title('System Presentation Distribution')
plt.xlabel('Score')
plt.ylabel('Frequency')
plt.show()


# Visualization of System Security Satisfaction
plt.figure(figsize=(10, 6))
sns.histplot(df['Security Satisfaction'], kde=True)
plt.title('System Security Satisfaction Distribution')
plt.xlabel('Score')
plt.ylabel('Frequency')
plt.show()


# Visualization of User Knowledge Satisfaction
plt.figure(figsize=(10, 6))
sns.histplot(df['User Knowledge Satisfaction'], kde=True)
plt.title('User Knowledge Satisfaction Distribution')
plt.xlabel('Score')
plt.ylabel('Frequency')
plt.show()

#Create a correlation heatmap for a subset of our dataset
# Import necessary libraries
import matplotlib.pyplot as plt
import seaborn as sns

# Define the subset of variables
subset_variables = ['System Navigation Satisfaction', 'System Presentation Satisfaction', 'Security Satisfaction', 'User Knowledge Satisfaction', 'Academic Service Delivery ']

# Create a subset DataFrame with only the selected variables
subset_df = df[subset_variables]

# Calculate the correlation matrix for the subset
correlation_matrix = subset_df.corr()

# Create the correlation heatmap
plt.figure(figsize=(10, 8))  # You can adjust the figure size as needed
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)
plt.title('Correlation Heatmap')
plt.show()

#Model implementation
# Define the threshold
threshold = 5

# Transform the dependent variable into 'Low' and 'High'
df['Academic Service Delivery '] = df['Academic Service Delivery '].apply(lambda x: 'Low' if x < threshold else 'High')

# Split the data into independent (X) and dependent (y) variables
X = df[['System Navigation Satisfaction','System Presentation Satisfaction', 'Security Satisfaction','User Knowledge Satisfaction']]
y = df['Academic Service Delivery ']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


from sklearn.preprocessing import StandardScaler

# Create a StandardScaler object
scaler = StandardScaler()

# Fit the scaler to your training data and transform it
X_train_scaled = scaler.fit_transform(X_train)

# Transform your test data using the same scaler
X_test_scaled = scaler.transform(X_test)

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score

# Create and fit the Logistic Regression model
model = LogisticRegression()
model.fit(X_train[['System Navigation Satisfaction']], y_train)
# Make predictions
y_pred = model.predict(X_test[['System Navigation Satisfaction']])

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred,zero_division=1)

print("Accuracy:", accuracy)
print("Classification Report:\n", report)


from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, roc_auc_score

# Make predictions with the best model
y_pred =model.predict(X_test[['System Navigation Satisfaction']])

# Compute the confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)

# Compute precision, recall, F1-score, and support for each class
precision, recall, f1_score, support = precision_recall_fscore_support(y_test, y_pred,zero_division=1)

# Compute the ROC AUC score (only for binary classification)
roc_auc = roc_auc_score(y_test,model.predict_proba(X_test[['System Navigation Satisfaction']])[:,1])

# Print the metrics
print("Confusion Matrix:\n", conf_matrix)
print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1_score)
print("Support:", support)
print("ROC AUC Score:", roc_auc)


#System Presentation Satisfaction classification model
# Create and fit the Logistic Regression model
model = LogisticRegression()
model.fit(X_train[['System Presentation Satisfaction']], y_train)
# Make predictions
y_pred = model.predict(X_test[['System Presentation Satisfaction']])

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred,zero_division=1)

print("Accuracy:", accuracy)
print("Classification Report:\n", report)


#System Presentation Satisfaction
from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, roc_auc_score

# Make predictions with the best model
y_pred =model.predict(X_test[['System Presentation Satisfaction']])

# Compute the confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)

# Compute precision, recall, F1-score, and support for each class
precision, recall, f1_score, support = precision_recall_fscore_support(y_test, y_pred,zero_division=1)

# Compute the ROC AUC score (only for binary classification)
roc_auc = roc_auc_score(y_test,model.predict_proba(X_test[['System Presentation Satisfaction']])[:,1])

# Print the metrics
print("Confusion Matrix:\n", conf_matrix)
print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1_score)
print("Support:", support)
print("ROC AUC Score:", roc_auc)


#System security Satisfaction
# Create and fit the Logistic Regression model
model = LogisticRegression()
model.fit(X_train[['Security Satisfaction']], y_train)
# Make predictions
y_pred = model.predict(X_test[['Security Satisfaction']])

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred,zero_division=1)

print("Accuracy:", accuracy)
print("Classification Report:\n", report)

#System security Satisfaction
from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, roc_auc_score

# Make predictions with the best model
y_pred =model.predict(X_test[['Security Satisfaction']])

# Compute the confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)

# Compute precision, recall, F1-score, and support for each class
precision, recall, f1_score, support = precision_recall_fscore_support(y_test, y_pred,zero_division=1)

# Compute the ROC AUC score (only for binary classification)
roc_auc = roc_auc_score(y_test,model.predict_proba(X_test[['Security Satisfaction']])[:,1])

# Print the metrics
print("Confusion Matrix:\n", conf_matrix)
print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1_score)
print("Support:", support)
print("ROC AUC Score:", roc_auc)


#User Knowledge Satisfaction 
# Create and fit the Logistic Regression model
model = LogisticRegression()
model.fit(X_train[['User Knowledge Satisfaction']], y_train)
# Make predictions
y_pred = model.predict(X_test[['User Knowledge Satisfaction']])

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred,zero_division=1)

print("Accuracy:", accuracy)
print("Classification Report:\n", report)


#User Knowledge Satisfaction 
from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, roc_auc_score

# Make predictions with the best model
y_pred =model.predict(X_test[['User Knowledge Satisfaction']])

# Compute the confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)

# Compute precision, recall, F1-score, and support for each class
precision, recall, f1_score, support = precision_recall_fscore_support(y_test, y_pred,zero_division=1)

# Compute the ROC AUC score (only for binary classification)
roc_auc = roc_auc_score(y_test,model.predict_proba(X_test[['User Knowledge Satisfaction']])[:,1])

# Print the metrics
print("Confusion Matrix:\n", conf_matrix)
print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1_score)
print("Support:", support)
print("ROC AUC Score:", roc_auc)


# Create and fit the Logistic Regression model
model = LogisticRegression()
model.fit(X_train, y_train)

# Get the coefficients (log odds) for the independent variables
log_odds = model.coef_[0]

# Get the feature names (independent variable names)
feature_names = X_train.columns

# Pair feature names with their corresponding log odds
feature_log_odds = list(zip(feature_names, log_odds))

# Print the feature log odds
for feature, log_odds_value in feature_log_odds:
    print(f"{feature}: {log_odds_value}")

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)

print("Accuracy:", accuracy)
print("Classification Report:\n", report)


#visualization of feature importance
import matplotlib.pyplot as plt
# Sort the features by log odds magnitude
feature_log_odds = sorted(feature_log_odds, key=lambda x: abs(x[1]), reverse=True)

# Extract feature names and log odds for plotting
sorted_feature_names, sorted_log_odds = zip(*feature_log_odds)

# Create a bar plot
plt.figure(figsize=(12, 6))
plt.barh(sorted_feature_names, sorted_log_odds)
plt.xlabel('Feature coefficients')
plt.title('Features in Logistic Regression Model')
plt.grid(axis='x', linestyle='--', alpha=0.6)

# Display the plot
plt.show()


#import necessary libraries
from sklearn.model_selection import GridSearchCV

# Define hyperparameters to tune
param_grid = {
    'C': [0.001, 0.01, 0.1, 1, 10, 100],  # Regularization parameter
    'penalty': ['l1', 'l2'],  # Regularization type
    'solver': ['liblinear']  # Suitable for small datasets
}

# Create and fit the GridSearchCV model
grid_search = GridSearchCV(LogisticRegression(), param_grid, cv=5)
grid_search.fit(X_train, y_train)

# Get the best parameters
best_params = grid_search.best_params_
best_model = grid_search.best_estimator_

# Re-fit the best model
best_model.fit(X_train, y_train)


from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, roc_auc_score

# Make predictions with the best model
y_pred =best_model.predict(X_test)

# Compute the confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)

# Compute precision, recall, F1-score, and support for each class
precision, recall, f1_score, support = precision_recall_fscore_support(y_test, y_pred,zero_division=1)

# Compute the ROC AUC score (only for binary classification)
roc_auc = roc_auc_score(y_test,best_model.predict_proba(X_test)[:,1])

# Print the metrics
print("Confusion Matrix:\n", conf_matrix)
print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1_score)
print("Support:", support)
print("ROC AUC Score:", roc_auc)
